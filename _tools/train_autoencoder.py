import argparse
import json
import os
from pathlib import Path

os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

import joblib
import numpy as np
import pandas as pd
from tqdm import tqdm
import tensorflow as tf
from keras.src import Model, Input
from keras.src.layers import Dense, LeakyReLU
# --- –ò–ó–ú–ï–ù–ï–ù–ò–ï ‚Ññ1: –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º ReduceLROnPlateau ---
from keras.src.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π ---
PROJECT_ROOT = Path(__file__).resolve().parents[1]
RESULTS_DIR = PROJECT_ROOT / "2_results"
FEATURES_DIR_PROCESSED = RESULTS_DIR / "features_processed"

def get_feature_columns(df: pd.DataFrame) -> list[str]:
    exclude_cols = {'datetime', 'ticker', 'label', 'meta_target', 'open', 'high', 'low', 'close', 'volume'}
    return [col for col in df.columns if col not in exclude_cols]

def create_autoencoder(input_dim: int, encoding_dim: int, leaky_relu_alpha: float, learning_rate: float) -> tuple[Model, Model]:
    input_layer = Input(shape=(input_dim,), name="input_features")
    encoded = Dense(input_dim // 2, activation='linear')(input_layer)
    encoded = LeakyReLU(negative_slope=leaky_relu_alpha)(encoded)
    encoded = Dense(input_dim // 4, activation='linear')(encoded)
    encoded = LeakyReLU(negative_slope=leaky_relu_alpha)(encoded)
    encoded = Dense(encoding_dim, activation='linear', name="encoded_vector")(encoded)
    decoded = Dense(input_dim // 4, activation='linear')(encoded)
    decoded = LeakyReLU(negative_slope=leaky_relu_alpha)(decoded)
    decoded = Dense(input_dim // 2, activation='linear')(decoded)
    decoded = LeakyReLU(negative_slope=leaky_relu_alpha)(decoded)
    decoded = Dense(input_dim, activation='linear', name="reconstructed_features")(decoded)
    
    autoencoder = Model(inputs=input_layer, outputs=decoded, name="Denoising_Autoencoder")
    encoder = Model(inputs=input_layer, outputs=encoded, name="Encoder")
    
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    autoencoder.compile(optimizer=optimizer, loss='mse')
    
    return autoencoder, encoder

def main(args):
    print("--- üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –®–£–ú–û–ü–û–î–ê–í–õ–Ø–Æ–©–ï–ì–û –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ (DAE) ---")
    print("üî•üî•üî• –í–ù–ò–ú–ê–ù–ò–ï: GPU –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω, –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –∏–¥—Ç–∏ –Ω–∞ CPU. üî•üî•üî•")
    
    model_dir = RESULTS_DIR / "autoencoders" / args.model_name
    model_dir.mkdir(parents=True, exist_ok=True)
    
    scaler_path = model_dir / "scaler.pkl"
    encoder_path = model_dir / "encoder.keras"
    metadata_path = model_dir / "metadata.json"

    # ... (–∫–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏...")
    all_files = list(FEATURES_DIR_PROCESSED.glob("*.csv"))
    df_list = [pd.read_csv(f) for f in tqdm(all_files)]
    full_df = pd.concat(df_list, ignore_index=True)
    full_df['datetime'] = pd.to_datetime(full_df['datetime'])
    full_df.sort_values(by='datetime', inplace=True)
    split_date = full_df['datetime'].max() - pd.DateOffset(years=1)
    train_df = full_df[full_df['datetime'] < split_date]
    feature_cols = get_feature_columns(train_df)
    X_train = train_df[feature_cols].values
    X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)

    print("–û–±—É—á–µ–Ω–∏–µ MinMaxScaler –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    joblib.dump(scaler, scaler_path)
    print(f"‚úÖ –°–∫–µ–π–ª–µ—Ä –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {scaler_path}")

    print(f"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ì–∞—É—Å—Å–æ–≤–∞ —à—É–º–∞ —Å —É—Ä–æ–≤–Ω–µ–º (—Å–∏–≥–º–∞): {args.noise_level}")
    noise = np.random.normal(loc=0.0, scale=args.noise_level, size=X_train_scaled.shape)
    X_train_noisy = X_train_scaled + noise
    X_train_noisy = np.clip(X_train_noisy, 0., 1.)

    print("–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã DAE...")
    autoencoder, encoder = create_autoencoder(
        input_dim=len(feature_cols),
        encoding_dim=args.encoding_dim,
        leaky_relu_alpha=args.leaky_relu_alpha,
        learning_rate=args.lr
    )
    autoencoder.summary()
    
    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)
    
    # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï ‚Ññ2: –°–æ–∑–¥–∞–µ–º callback –¥–ª—è –ø–æ–Ω–∏–∂–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è ---
    reduce_lr_callback = ReduceLROnPlateau(
        monitor='val_loss', 
        factor=0.2,      # –£–º–µ–Ω—å—à–∞–µ–º LR –≤ 5 —Ä–∞–∑
        patience=5,      # –ñ–¥–µ–º 5 —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π
        verbose=1,       # –°–æ–æ–±—â–∞–µ–º –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
        min_lr=1e-6      # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    )
    
    print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è DAE (—à—É–º -> —á–∏—Å—Ç—ã–µ)...")
    history = autoencoder.fit(
        X_train_noisy,
        X_train_scaled,
        epochs=args.epochs,
        batch_size=args.batch_size,
        shuffle=True,
        validation_split=0.1,
        # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï ‚Ññ3: –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π callback –≤ —Å–ø–∏—Å–æ–∫ ---
        callbacks=[early_stopping, reduce_lr_callback],
        verbose=1
    )

    min_val_loss = min(history.history.get('val_loss', [np.inf]))
    print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π val_loss: {min_val_loss:.6f}")
    
    encoder.save(encoder_path)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —ç–Ω–∫–æ–¥–µ—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {encoder_path}")
    
    metadata = {
        "model_name": args.model_name,
        "type": "DenoisingAutoencoder",
        "noise_level": args.noise_level,
        "source_features_count": len(feature_cols),
        "encoded_dim": args.encoding_dim,
        "min_validation_loss": min_val_loss,
        "source_feature_names": feature_cols
    }
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=4)
    print(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {metadata_path}")
    print("\nüéâ –ü–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è DAE —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train a Denoising Autoencoder for feature extraction.")
    parser.add_argument("--model_name", type=str, required=True, help="A unique name for this autoencoder model.")
    parser.add_argument("--encoding_dim", type=int, default=30, help="The dimensionality of the encoded representation.")
    parser.add_argument("--epochs", type=int, default=200, help="Maximum number of training epochs.")
    parser.add_argument("--batch_size", type=int, default=8192, help="Batch size for training.")
    parser.add_argument("--leaky_relu_alpha", type=float, default=0.2, help="Alpha parameter for LeakyReLU activation.")
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate for the Adam optimizer.")
    parser.add_argument("--noise_level", type=float, default=0.1, help="Standard deviation of Gaussian noise to add to the input data.")
    
    args = parser.parse_args()
    main(args)