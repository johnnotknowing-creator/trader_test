from _core.paths import PROJECT_ROOT, DATA_DIR, RESULTS_DIR, TOOLS_DIR, CONFIGS_DIR, ensure_dirs, load_dotenv_if_exists
from _core.libs import *
load_dotenv_if_exists()
ensure_dirs()
# _tools/feature_selector_perm.py
from _core.data_loader import load_data
from _core.feature_generator import create_features
from sklearn.model_selection import TimeSeriesSplit

CONFIG = {
    "HORIZON": 20,
    "TOP_N_FEATURES": 25,
    "PERMUTATION_REPEATS": 10,   # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ PI –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    "N_SPLITS": 5,               # —á–∏—Å–ª–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–æ–ª–¥–æ–≤
    "PURGE_GAP_DAYS": 20,        # –∑–∞—â–∏—Ç–∞ –æ—Ç —É—Ç–µ—á–µ–∫ –º–µ–∂–¥—É train –∏ val
    "CORR_THRESHOLD": 0.90,      # –ø–æ—Ä–æ–≥ –≤–∑–∞–∏–º–Ω–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    "MIN_LEN_TICKER": 500,       # –º–∏–Ω–∏–º—É–º —Å—Ç—Ä–æ–∫ –Ω–∞ —Ç–∏–∫–µ—Ä
    "BOOTSTRAPS": 1,             # –ª—ë–≥–∫–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å: —Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–≥–æ–Ω–æ–≤ —Å —Å—É–±—Å–µ–º–ø–ª–∏–Ω–≥–æ–º –≤—Ä–µ–º–µ–Ω–∏
    "BOOTSTRAP_FRACTION": 1.0    # –¥–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –≤–æ –≤—Ä–µ–º–µ–Ω–∏ (<=1.0). 1.0 = –≤—ã–∫–ª—é—á–µ–Ω–æ
}

def _build_index_df(data_dir):
    idx_raw = load_data(os.path.join(data_dir, "IMOEX.csv"))
    if idx_raw is None or idx_raw.empty:
        raise FileNotFoundError("–ù–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç 1_data/IMOEX.csv")
    idx = idx_raw.copy()
    for p in [10, 20, 50]:
        idx[f'index_roc_{p}'] = ta.roc(idx['close'], length=p)
    idx.reset_index(inplace=True)
    return idx

def _collect_training_frame(universe_path, data_dir, index_df, horizon, min_len=500):
    try:
        tickers_df = pd.read_csv(universe_path)
        tickers = tickers_df['ticker'].dropna().unique()
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º —Ç–∏–∫–µ—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {universe_path}")
        return None

    all_train = []
    for ticker in tqdm(tickers, desc="–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞"):
        df = load_data(os.path.join(data_dir, f"{ticker}_D1_MOEX.csv"))
        if df is None or len(df) < min_len:
            continue
        feats = create_features(df, index_df=index_df, horizon=horizon)
        if feats.empty:
            continue
        feats['datetime'] = pd.to_datetime(feats['datetime'])
        # –±–µ—Ä—ë–º ¬´—Ç—Ä–µ–π–Ω –±–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≥–æ–¥–∞¬ª
        cutoff = feats['datetime'].max() - relativedelta(years=1)
        train_df = feats[feats['datetime'] < cutoff].copy()
        if len(train_df) == 0:
            continue
        all_train.append(train_df)
    if not all_train:
        return None
    combined = pd.concat(all_train, ignore_index=True)
    return combined

def _purge_train_indices(tr_idx, va_idx, gap):
    """–£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 'gap' –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏–∑ train, —á—Ç–æ–±—ã —Ä–∞–∑–Ω–µ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç –Ω–∞—á–∞–ª–∞ val."""
    if gap <= 0:
        return tr_idx
    max_train = tr_idx.max()
    min_val = va_idx.min()
    # –µ—Å–ª–∏ –º–µ–∂–¥—É max_train –∏ min_val –º–µ–Ω—å—à–µ gap, –æ–±—Ä–µ–∂–µ–º train
    cutoff = min_val - gap
    return tr_idx[tr_idx <= cutoff]

def _permutation_importance_oos(X, y, n_splits=5, gap=20, repeats=10, scoring='f1_weighted',
                                model_kwargs=None, n_jobs=-1, boots=1, bootstrap_frac=1.0, random_state=42):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–º–∏ –ø–æ OOS-—Ñ–æ–ª–¥–∞–º –≤–∞–∂–Ω–æ—Å—Ç—è–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    –ú–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –ª—ë–≥–∫–∏–π –±—É—Ç—Å—Ç—Ä–∞–ø –ø–æ –≤—Ä–µ–º–µ–Ω–∏: –±–µ—Ä—ë–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –∫—É—Å–æ–∫ —Ä—è–¥–∞.
    """
    rng = np.random.default_rng(random_state)
    feats = X.columns.tolist()
    agg_list = []

    for b in range(boots):
        Xb, yb = X, y
        if bootstrap_frac < 1.0:
            # time-subset: –±–µ—Ä—ë–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ –¥–ª–∏–Ω–æ–π frac
            n = len(X)
            win = int(max(100, np.floor(n * bootstrap_frac)))
            start = int(np.floor((n - win) / 2))
            Xb = X.iloc[start:start+win]
            yb = y.iloc[start:start+win]

        tscv = TimeSeriesSplit(n_splits=n_splits)
        fold_imps = []

        for fold, (tr_idx_raw, va_idx) in enumerate(tscv.split(Xb)):
            tr_idx = _purge_train_indices(tr_idx_raw, va_idx, gap)
            if len(tr_idx) == 0:
                continue

            X_tr, y_tr = Xb.iloc[tr_idx], yb.iloc[tr_idx]
            X_va, y_va = Xb.iloc[va_idx], yb.iloc[va_idx]

            # LightGBM –Ω–∞ train
            params = {"random_state": (random_state + b + fold), "n_jobs": -1}
            if model_kwargs:
                params.update(model_kwargs)
            model = lgb.LGBMClassifier(**params)
            model.fit(X_tr, y_tr, callbacks=[lgb.log_evaluation(period=0)])

            # Permutation Importance –Ω–∞ –í–ê–õ–ò–î–ê–¶–ò–ò (OOS)
            result = permutation_importance(
                model, X_va, y_va,
                n_repeats=repeats, random_state=(random_state + 123*b + fold),
                scoring=scoring, n_jobs=n_jobs
            )
            fold_imps.append(pd.DataFrame({
                "feature": feats,
                f"imp_fold{fold}": result.importances_mean
            }))

        if not fold_imps:
            continue

        imp_df = fold_imps[0]
        for extra in fold_imps[1:]:
            imp_df = imp_df.merge(extra, on="feature", how="inner")
        imp_df["imp_mean"] = imp_df.filter(like="imp_fold").mean(axis=1)
        imp_df["boot_id"] = b
        agg_list.append(imp_df[["feature", "imp_mean"]])

    if not agg_list:
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—á–∏—Ç–∞—Ç—å permutation importance –Ω–∏ –Ω–∞ –æ–¥–Ω–æ–º —Ñ–æ–ª–¥–µ/–±—É—Ç—Å—Ç—Ä–∞–ø–µ.")

    out = pd.concat(agg_list, ignore_index=True)
    out = out.groupby("feature", as_index=False)["imp_mean"].mean().sort_values("imp_mean", ascending=False)
    return out

def _correlation_filter(feature_order, corr_df, threshold=0.90):
    selected = []
    for f in feature_order:
        if f not in corr_df.columns:
            selected.append(f)
            continue
        ok = True
        for s in selected:
            if s in corr_df.columns and abs(corr_df.loc[f, s]) >= threshold:
                ok = False
                break
        if ok:
            selected.append(f)
    return selected

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ—Ç–æ–¥–æ–º OOS Permutation Importance (time-aware).")
    # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —Å feature_selector.py
    parser.add_argument('--model_name', type=str, required=True, help='–£–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
    parser.add_argument('--universe_file', type=str, required=True, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º —Ç–∏–∫–µ—Ä–æ–≤')

    # –î–æ–ø. –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    parser.add_argument('--top_n', type=int, default=CONFIG["TOP_N_FEATURES"], help='–°–∫–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å')
    parser.add_argument('--scoring', type=str, default='f1_weighted', help='–ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è permutation_importance')
    parser.add_argument('--n_splits', type=int, default=CONFIG["N_SPLITS"], help='–ß–∏—Å–ª–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–æ–ª–¥–æ–≤')
    parser.add_argument('--purge_gap', type=int, default=CONFIG["PURGE_GAP_DAYS"], help='–†–∞–∑—Ä—ã–≤ (–¥–Ω–µ–π) –º–µ–∂–¥—É train –∏ val')
    parser.add_argument('--repeats', type=int, default=CONFIG["PERMUTATION_REPEATS"], help='–ü–æ–≤—Ç–æ—Ä—ã permutation importance')
    parser.add_argument('--corr_threshold', type=float, default=CONFIG["CORR_THRESHOLD"], help='–ü–æ—Ä–æ–≥ |œÅ| –¥–ª—è –∫–æ—Ä—Ä-—Ñ–∏–ª—å—Ç—Ä–∞')
    parser.add_argument('--min_len_ticker', type=int, default=CONFIG["MIN_LEN_TICKER"], help='–ú–∏–Ω. –¥–ª–∏–Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Ç–∏–∫–µ—Ä–∞')
    parser.add_argument('--boots', type=int, default=CONFIG["BOOTSTRAPS"], help='–ë—É—Ç—Å—Ç—Ä–∞–ø—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å)')
    parser.add_argument('--bootstrap_frac', type=float, default=CONFIG["BOOTSTRAP_FRACTION"], help='–î–æ–ª—è –≤—Ä–µ–º–µ–Ω–∏ –≤ –±—É—Ç—Å—Ç—Ä–∞–ø–µ (<=1.0)')
    parser.add_argument('--n_jobs', type=int, default=-1, help='n_jobs –¥–ª—è permutation_importance')
    args = parser.parse_args()

    data_dir = os.path.join(PROJECT_ROOT, "1_data")
    results_dir = os.path.join(PROJECT_ROOT, "2_results")
    os.makedirs(results_dir, exist_ok=True)

    # 0) –ò–Ω–¥–µ–∫—Å –∏ —Å–±–æ—Ä –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–≥–æ train-—Ñ—Ä–µ–π–º–∞
    print("--- –≠—Ç–∞–ø 0: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∏–Ω–¥–µ–∫—Å—É IMOEX ---")
    index_df = _build_index_df(data_dir)

    print("\n--- –≠—Ç–∞–ø 1: –°–±–æ—Ä —Ç—Ä–µ–π–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ ---")
    universe_path = os.path.join(PROJECT_ROOT, args.universe_file)
    combined_df = _collect_training_frame(universe_path, data_dir, index_df,
                                          horizon=CONFIG["HORIZON"], min_len=args.min_len_ticker)
    if combined_df is None or combined_df.empty:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."); exit(1)

    # –§–æ—Ä–º–∏—Ä—É–µ–º X / y
    feature_columns = [c for c in combined_df.columns if c not in ['datetime', 'target']]
    X = combined_df[feature_columns]
    y = combined_df['target']

    # 2) OOS Permutation Importance –ø–æ —Ñ–æ–ª–¥–∞–º –≤—Ä–µ–º–µ–Ω–∏ (+ –ª—ë–≥–∫–∏–π –±—É—Ç—Å—Ç—Ä–∞–ø –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏)
    print("\n--- –≠—Ç–∞–ø 2: –†–∞—Å—á—ë—Ç OOS Permutation Importance ---")
    perm_df = _permutation_importance_oos(
        X, y,
        n_splits=args.n_splits,
        gap=args.purge_gap,
        repeats=args.repeats,
        scoring=args.scoring,
        model_kwargs={"n_jobs": -1},
        n_jobs=args.n_jobs,
        boots=args.boots,
        bootstrap_frac=args.bootstrap_frac,
        random_state=42
    )

    # 3) –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä (–ø–æ –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º)
    print("\n--- –≠—Ç–∞–ø 3: –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä ---")
    # —á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞—Ç—å –Ω–∞ —Ä–µ–¥–∫–∏—Ö —Ñ–∏—á–∞—Ö, –æ—Å—Ç–∞–≤–∏–º —Ç–µ, —á—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤ X
    ordered_feats = [f for f in perm_df['feature'].tolist() if f in X.columns]
    corr = X[ordered_feats].corr().abs()
    filtered = _correlation_filter(ordered_feats, corr, threshold=args.corr_threshold)

    # 4) –¢–æ–ø-N –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    top_features = filtered[:args.top_n]
    imp_report = perm_df.copy()
    imp_report['selected'] = imp_report['feature'].isin(top_features).astype(int)

    out_json = os.path.join(results_dir, f"selected_features_{args.model_name}.json")
    with open(out_json, 'w') as f:
        json.dump(top_features, f, indent=4)
    print(f"\n‚úÖ –°–ø–∏—Å–æ–∫ –∏–∑ {len(top_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {out_json}")

    out_csv = os.path.join(results_dir, f"perm_importance_{args.model_name}.csv")
    imp_report.to_csv(out_csv, index=False)
    print(f"üìÑ –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ –≤–∞–∂–Ω–æ—Å—Ç—è–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {out_csv}")

    print("\n--- –¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---")
    print(pd.DataFrame({"feature": top_features}).to_string(index=False))
