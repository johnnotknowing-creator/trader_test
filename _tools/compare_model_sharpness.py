# _tools/compare_model_sharpness.py
import argparse
import numpy as np
import pandas as pd
import os
from pathlib import Path
import warnings
import json
from tqdm import tqdm

# --- –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π ---
warnings.filterwarnings("ignore", category=UserWarning)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf

# --- –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π ---
try:
    PROJECT_DIR = Path(__file__).resolve().parent.parent
except NameError:
    PROJECT_DIR = Path.cwd()

RESULTS_DIR = PROJECT_DIR / "2_results"
MODELS_DIR = RESULTS_DIR / "models"
TFRECORDS_DIR = RESULTS_DIR
# --- –ö–æ–Ω–µ—Ü –±–ª–æ–∫–∞ ---

# --- –ò–°–ü–†–ê–í–õ–ï–ù–û: –§—É–Ω–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Ç–µ–ø–µ—Ä—å –∏–¥–µ–Ω—Ç–∏—á–Ω—ã —Å–∫—Ä–∏–ø—Ç–∞–º –æ–±—É—á–µ–Ω–∏—è ---
def parse_tfrecord_fn(example, lookback, n_features):
    """–ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∏–∑ TFRecord —Ñ–∞–π–ª–∞, –∑–Ω–∞—è –µ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É."""
    feature_description = {
        'sequence': tf.io.FixedLenFeature([], tf.string),
        'target': tf.io.FixedLenFeature([], tf.int64)
    }
    example = tf.io.parse_single_example(example, feature_description)
    sequence = tf.io.parse_tensor(example['sequence'], out_type=tf.float32)
    label = example['target']
    
    sequence.set_shape([lookback, n_features])
    # --- –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫—É –≤ one-hot —Ñ–æ—Ä–º–∞—Ç ---
    label = tf.one_hot(label, depth=3)
    label.set_shape([3])
    return sequence, label

def load_tfrecord_dataset(file_path, batch_size, lookback, n_features):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ TFRecord —Ñ–∞–π–ª–∞."""
    dataset = tf.data.TFRecordDataset(str(file_path))
    parser = lambda x: parse_tfrecord_fn(x, lookback, n_features)
    dataset = dataset.map(parser, num_parallel_calls=tf.data.AUTOTUNE)
    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
# --- –ö–û–ù–ï–¶ –ë–õ–û–ö–ê –ò–°–ü–†–ê–í–õ–ï–ù–ò–ô ---


def measure_sharpness_for_model(model_name: str, data_model_name: str, args: argparse.Namespace):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å –∏ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏–∑–º–µ—Ä—è–µ—Ç –µ–µ loss, accuracy –∏ sharpness."""
    model_path = MODELS_DIR / model_name / "model.keras"
    tfrecord_base_path = TFRECORDS_DIR / f"tfrecord_{data_model_name}"
    meta_path = tfrecord_base_path / "metadata.json"

    try:
        model = tf.keras.models.load_model(model_path)
        with open(meta_path, 'r') as f: metadata = json.load(f)
    except Exception as e:
        print(f"  [WARN] –ü—Ä–æ–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ {model_name}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã. –û—à–∏–±–∫–∞: {e}")
        return None
        
    try:
        seq_len, n_features = metadata['lookback'], metadata['n_features']
    except KeyError:
        print(f"  [WARN] –ü—Ä–æ–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ {model_name}: –≤ metadata.json –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–∏ 'lookback' –∏–ª–∏ 'n_features'.")
        return None

    val_file = tfrecord_base_path / "test.tfrecord"
    # --- –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–µ—Ä–µ–¥–∞–µ–º seq_len –∏ n_features –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ ---
    val_dataset = load_tfrecord_dataset(str(val_file), args.batch_size, seq_len, n_features)
    
    # --- –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–¥–∞–ª–µ–Ω–∞ –ª–∏—à–Ω—è—è –ª–æ–≥–∏–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è. –î–∞–Ω–Ω—ã–µ –≤ TFRecord —É–∂–µ –æ—Ç–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω—ã. ---
    
    # --- –û—Ü–µ–Ω–∫–∞ –±–∞–∑–æ–≤–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ---
    loss_base, acc_base = model.evaluate(val_dataset, verbose=0)
    
    perturbed_losses = []
    center_weights = model.get_weights()
    
    # --- –†–∞—Å—á–µ—Ç "—Ä–µ–∑–∫–æ—Å—Ç–∏" ---
    for _ in range(args.n_probes):
        noise = [np.random.normal(0, 1, w.shape) for w in center_weights]
        for n, w in zip(noise, center_weights):
            n_norm, w_norm = np.linalg.norm(n.flatten()), np.linalg.norm(w.flatten())
            if n_norm > 0 and w_norm > 0:
                n *= (w_norm / n_norm) * args.noise_scale
        perturbed_weights = [w + n for w, n in zip(center_weights, noise)]
        model.set_weights(perturbed_weights)
        loss_perturbed, _ = model.evaluate(val_dataset, verbose=0)
        perturbed_losses.append(loss_perturbed)

    sharpness_index = np.mean(perturbed_losses) - loss_base
    
    return {"model_name": model_name, "min_loss": loss_base, "accuracy": acc_base, "sharpness": sharpness_index}

def main(args):
    model_prefix = args.model_prefix
    print(f"--- –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑–∫–æ—Å—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º: '{model_prefix}' ---")
    
    model_dirs = sorted([d for d in MODELS_DIR.glob(f"{model_prefix}*") if d.is_dir()])
    if not model_dirs:
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞–ø–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ—Ñ–∏–∫—Å–∞ '{model_prefix}' –≤ {MODELS_DIR}"); return
        
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(model_dirs)} –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.")
    
    if args.data_model_name:
        print(f"–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –æ—Ç: '{args.data_model_name}' (–¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π)")
    else:
        print("–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏.")

    all_results = []
    for model_dir in tqdm(model_dirs, desc="–ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–µ–π"):
        data_source_name = args.data_model_name if args.data_model_name else model_dir.name
        
        metrics = measure_sharpness_for_model(model_dir.name, data_source_name, args)
        if metrics:
            all_results.append(metrics)

    if not all_results:
        print("\n–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."); return
        
    results_df = pd.DataFrame(all_results).set_index('model_name')
    
    print("\n--- üìä –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è) ---")
    print(results_df.to_string(float_format="%.4f", columns=["min_loss", "accuracy", "sharpness"]))
    
    best_loss_model = results_df['min_loss'].idxmin()
    best_sharpness_model = results_df['sharpness'].idxmin()
    best_accuracy_model = results_df['accuracy'].idxmax()
    
    print("\n--- üèÜ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ---")
    print(f"–ú–æ–¥–µ–ª—å —Å —Å–∞–º—ã–º –Ω–∏–∑–∫–∏–º loss (–ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ): {best_loss_model} ({results_df.loc[best_loss_model, 'min_loss']:.4f})")
    print(f"–ú–æ–¥–µ–ª—å —Å —Å–∞–º–æ–π –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é (best accuracy): {best_accuracy_model} ({results_df.loc[best_accuracy_model, 'accuracy']:.4f})")
    print(f"–ú–æ–¥–µ–ª—å —Å –Ω–∞–∏–º–µ–Ω—å—à–µ–π —Ä–µ–∑–∫–æ—Å—Ç—å—é (—Å–∞–º–∞—è —É—Å—Ç–æ–π—á–∏–≤–∞—è): {best_sharpness_model} ({results_df.loc[best_sharpness_model, 'sharpness']:.4f})")
    
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑–∫–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π.")
    parser.add_argument('--model_prefix', type=str, required=True)
    parser.add_argument('--data_model_name', type=str, default=None, help="–ò–º—è –º–æ–¥–µ–ª–∏, —á—å–∏ –¥–∞–Ω–Ω—ã–µ (TFRecord) –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∫–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ.")
    parser.add_argument('--batch_size', type=int, default=8192)
    parser.add_argument('--n_probes', type=int, default=10)
    parser.add_argument('--noise_scale', type=float, default=1e-3)
    args = parser.parse_args()
    main(args)