import argparse
import joblib
from pathlib import Path
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from tqdm import tqdm
import json

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π ---
PROJECT_ROOT = Path(__file__).resolve().parents[1]
RESULTS_DIR = PROJECT_ROOT / "2_results"
FEATURES_DIR_PROCESSED = RESULTS_DIR / "features_processed"

def get_feature_columns(df: pd.DataFrame) -> list[str]:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏."""
    exclude_cols = {'datetime', 'ticker', 'label', 'meta_target', 'open', 'high', 'low', 'close', 'volume'}
    return [col for col in df.columns if col not in exclude_cols]

def main(args):
    print("--- üî¨ –ó–∞–ø—É—Å–∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (PCA) ---")
    
    output_dir = RESULTS_DIR / "features_pca" / args.model_name
    output_dir.mkdir(parents=True, exist_ok=True)
    scaler_path = output_dir / "scaler.pkl"
    pca_path = output_dir / "pca.pkl"
    metadata_path = output_dir / "metadata.json"
    print(f"–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")

    print("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏...")
    all_files = list(FEATURES_DIR_PROCESSED.glob("*.csv"))
    if not all_files:
        print(f"‚ùå –û—à–∏–±–∫–∞: –í –ø–∞–ø–∫–µ {FEATURES_DIR_PROCESSED} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤.")
        return

    df_list = [pd.read_csv(f) for f in tqdm(all_files)]
    full_df = pd.concat(df_list, ignore_index=True)
    full_df['datetime'] = pd.to_datetime(full_df['datetime'])
    full_df.sort_values(by=['ticker', 'datetime'], inplace=True)
    
    split_date = full_df['datetime'].max() - pd.DateOffset(years=1)
    train_df = full_df[full_df['datetime'] < split_date]
    
    feature_cols = get_feature_columns(train_df)
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(feature_cols)} –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è PCA.")
    
    X_train = train_df[feature_cols].values
    X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)
    
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    
    print(f"–û–±—É—á–µ–Ω–∏–µ PCA –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è {args.n_components} –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç...")
    pca = PCA(n_components=args.n_components)
    pca.fit(X_train_scaled)
    
    explained_variance = pca.explained_variance_ratio_
    print(f"–í—Å–µ–≥–æ {args.n_components} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –æ–±—ä—è—Å–Ω—è—é—Ç: {np.sum(explained_variance):.2%} –≤—Å–µ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏.")

    joblib.dump(scaler, scaler_path)
    joblib.dump(pca, pca_path)
    print(f"‚úÖ –°–∫–µ–π–ª–µ—Ä –∏ PCA –æ–±—É—á–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")

    # --- üëá –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ü—Ä–∏–º–µ–Ω—è–µ–º PCA –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ —Ñ–∞–π–ª–∞–º, –∞ –Ω–µ –≤ –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π üëá ---
    print("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ PCA –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ —Ñ–∞–π–ª–∞–º...")
    pca_feature_names = [f"PCA_{i}" for i in range(args.n_components)]
    
    for ticker, group_df in tqdm(full_df.groupby('ticker'), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–∏–∫–µ—Ä–æ–≤"):
        meta_cols = [col for col in ['datetime', 'ticker', 'label'] if col in group_df.columns]
        df_meta = group_df[meta_cols].reset_index(drop=True)

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ —Ñ–∏—á–∏ –Ω–∞ –º–µ—Å—Ç–µ –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        X_group = group_df[feature_cols].copy()
        missing_features = set(feature_cols) - set(X_group.columns)
        if missing_features:
            for feature in missing_features:
                X_group[feature] = 0
        X_group = X_group[feature_cols]

        X_group_np = np.nan_to_num(X_group.values, nan=0.0, posinf=0.0, neginf=0.0)
        
        X_group_scaled = scaler.transform(X_group_np)
        X_pca = pca.transform(X_group_scaled)
        
        df_pca = pd.DataFrame(X_pca, columns=pca_feature_names)
        
        final_df = pd.concat([df_meta, df_pca], axis=1)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª
        output_file_path = output_dir / f"{ticker}.csv"
        final_df.to_csv(output_file_path, index=False)
    # --- üëÜ –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–Ø üëÜ ---
        
    metadata = { "model_name": args.model_name, "type": "PCA", "n_components": args.n_components, "explained_variance_ratio": explained_variance.tolist() }
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=4)

    print(f"\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –∏–∑ {args.n_components} –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Extract orthogonal features using PCA.")
    parser.add_argument("--model_name", type=str, required=True, help="A unique name for this PCA model/run.")
    parser.add_argument("--n_components", type=int, default=50, help="Number of principal components to extract.")
    args = parser.parse_args()
    main(args)