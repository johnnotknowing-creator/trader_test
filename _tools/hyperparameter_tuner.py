# _tools/hyperparameter_tuner.py
import os
import warnings

os.environ['TF_ENABLE_XLA'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings("ignore", category=UserWarning)

import argparse
import json
from pathlib import Path
import numpy as np
import tensorflow as tf
import optuna
from optuna.pruners import HyperbandPruner
from optuna.integration import TFKerasPruningCallback

from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, GRU, Bidirectional, Dense, Dropout, Attention,
    Add, LayerNormalization, GlobalAveragePooling1D, LeakyReLU, Conv1D
)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2

# –í–∫–ª—é—á–∞–µ–º —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —Ç–µ—Å—Ç–∞ –Ω–∞ tf-nightly
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# --- –ü—É—Ç–∏ ---
try:
    PROJECT_ROOT = Path(__file__).resolve().parents[1]
except NameError:
    PROJECT_ROOT = Path.cwd()

RESULTS_DIR = PROJECT_ROOT / "2_results"
TFRECORDS_DIR = RESULTS_DIR

# --- –ù–û–í–´–ô –ö–û–õ–õ–ë–≠–ö –î–õ–Ø –°–û–•–†–ê–ù–ï–ù–ò–Ø –õ–£–ß–®–ò–• –ü–ê–†–ê–ú–ï–¢–†–û–í ---
def save_best_trial_callback(study: optuna.study.Study, trial: optuna.trial.FrozenTrial):
    """
    Callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–∏—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ JSON —Ñ–∞–π–ª –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏,
    –µ—Å–ª–∏ —Ç–µ–∫—É—â–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è –ø–æ–∫–∞–∑–∞–ª–∞ –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
    """
    if study.best_trial.number == trial.number:
        print(f"\n[Callback] –ù–∞–π–¥–µ–Ω –Ω–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (trial #{trial.number}): "
              f"val_accuracy = {trial.value:.5f}. –°–æ—Ö—Ä–∞–Ω—è—é –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")

        RESULTS_DIR.mkdir(parents=True, exist_ok=True)
        params_path = RESULTS_DIR / f"best_params_{study.study_name}.json"
        best_params = study.best_trial.params

        try:
            with open(params_path, 'w') as f:
                json.dump(best_params, f, indent=4)
            print(f"[Callback] ‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {params_path}")
        except Exception as e:
            print(f"[Callback] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
# --- –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –ë–õ–û–ö–ê ---


# --- –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ ---
def parse_tfrecord_fn(example, lookback, n_features):
    feature_description = {'sequence': tf.io.FixedLenFeature([], tf.string), 'target': tf.io.FixedLenFeature([], tf.int64)}
    example = tf.io.parse_single_example(example, feature_description)
    sequence = tf.io.parse_tensor(example['sequence'], out_type=tf.float32)
    label = example['target']
    sequence.set_shape([lookback, n_features])
    label = tf.one_hot(label, depth=3)
    label.set_shape([3])
    return sequence, label

def load_tfrecord_dataset(file_path, batch_size, lookback, n_features):
    dataset = tf.data.TFRecordDataset(str(file_path))
    dataset = dataset.map(lambda x: parse_tfrecord_fn(x, lookback, n_features), num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.cache()
    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# --- –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ ---
def create_tuned_model(seq_len, n_features, trial):
    conv_filters = trial.suggest_categorical("conv_filters", [42, 60, 72, 88, 104, 120, 136, 152])
    kernel_size = trial.suggest_categorical("kernel_size", [5, 6, 7, 8, 9, 10, 11])
    gru_units_1 = trial.suggest_categorical("gru_units_1", [6, 7, 8, 9, 10, 11, 12, 13])
    gru_units_2 = trial.suggest_categorical("gru_units_2", [7, 8, 9, 10, 11, 12, 13, 14])
    dense_units = trial.suggest_categorical("dense_units", [32, 36, 40, 44, 48, 52])

    dropout_conv = trial.suggest_float("dropout_conv", 0.03, 0.2)
    dropout_gru = trial.suggest_float("dropout_gru", 0.01, 0.2)
    l2_reg = trial.suggest_float("l2_reg", 1e-8, 1e-5, log=True)
    alpha = trial.suggest_float("lrelu_neg_slope", 0.01, 0.4)

    inputs = Input(shape=(seq_len, n_features))

    x = Conv1D(filters=conv_filters, kernel_size=kernel_size, padding='causal', kernel_regularizer=l2(l2_reg))(inputs)
    x = LayerNormalization()(x)
    x = LeakyReLU(alpha)(x)
    x = Dropout(dropout_conv)(x)

    x = Bidirectional(GRU(gru_units_1, return_sequences=True, kernel_regularizer=l2(l2_reg)))(x)
    x = Dropout(dropout_gru)(x)
    x = Bidirectional(GRU(gru_units_2, return_sequences=True, kernel_regularizer=l2(l2_reg)))(x)
    res_x = Dropout(dropout_gru)(x)

    attn_out = Attention()([res_x, res_x])
    x = Add()([res_x, attn_out])
    x = LayerNormalization()(x)
    x = GlobalAveragePooling1D()(x)

    x = Dense(dense_units, kernel_regularizer=l2(l2_reg))(x)
    x = LeakyReLU(alpha)(x)
    x = Dropout(dropout_gru)(x)

    outputs = Dense(3, activation='softmax', dtype='float32')(x)

    return Model(inputs=inputs, outputs=outputs)

# --- –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è "—Ü–µ–ª–∏" –¥–ª—è Optuna ---
def objective(trial, args, metadata, train_dataset, val_dataset):
    tf.keras.backend.clear_session()

    model = create_tuned_model(metadata['lookback'], metadata['n_features'], trial)

    lr = trial.suggest_float("lr", 1e-4, 1e-2, log=True)
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    pruning_callback = TFKerasPruningCallback(trial, 'val_accuracy')
    early_stopping = EarlyStopping(monitor='val_accuracy', mode='max', patience=12, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(
        monitor='val_accuracy',
        factor=0.2,
        patience=5,
        min_lr=1e-6,
        verbose=0
    )
    callbacks = [early_stopping, pruning_callback, reduce_lr]

    history = model.fit(
        train_dataset,
        epochs=70,
        validation_data=val_dataset,
        callbacks=callbacks,
        class_weight={int(k): v for k, v in metadata['class_weights'].items()},
        verbose=0
    )

    best_epoch_index = np.argmax(history.history.get('val_accuracy', [0]))
    best_val_accuracy = history.history['val_accuracy'][best_epoch_index]
    best_val_loss = history.history['val_loss'][best_epoch_index]

    trial.set_user_attr("val_loss", best_val_loss)

    return best_val_accuracy

def print_trial_callback(study, trial):
    is_pruned = "PRUNED" if trial.state == optuna.trial.TrialState.PRUNED else ""
    current_val_acc = trial.value
    if current_val_acc is None:
        current_val_acc = float('nan')

    current_val_loss = trial.user_attrs.get('val_loss', float('nan'))

    best_trial = study.best_trial
    best_val_acc = best_trial.value
    best_val_loss = best_trial.user_attrs.get('val_loss', float('nan'))

    print(f"Trial {trial.number:3d} finished. "
          f"Current: [Acc: {current_val_acc:.4f}, Loss: {current_val_loss:.4f}]. "
          f"Best (Trial #{best_trial.number}): [Acc: {best_val_acc:.4f}, Loss: {best_val_loss:.4f}]. {is_pruned}")

# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ---
def main(args):
    tfrecord_base_path = TFRECORDS_DIR / f"tfrecord_{args.model_name}"
    meta_path = tfrecord_base_path / "metadata.json"

    try:
        with open(meta_path, 'r') as f:
            metadata = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {meta_path}")
        return

    train_dataset = load_tfrecord_dataset(tfrecord_base_path / "train.tfrecord", args.batch_size, metadata['lookback'], metadata['n_features'])
    val_dataset = load_tfrecord_dataset(tfrecord_base_path / "test.tfrecord", args.batch_size, metadata['lookback'], metadata['n_features'])

    pruner = HyperbandPruner()
    # ---> –ò–ó–ú–ï–ù–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º study_name <---
    study = optuna.create_study(
        direction='maximize',
        pruner=pruner,
        study_name=args.model_name
    )

    # ---> –ò–ó–ú–ï–ù–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—à –Ω–æ–≤—ã–π callback –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è <---
    study.optimize(
        lambda trial: objective(trial, args, metadata, train_dataset, val_dataset),
        n_trials=args.n_trials,
        show_progress_bar=False,
        callbacks=[print_trial_callback, save_best_trial_callback]
    )

    print("\n--- üèÜ –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω! ---")

    pruned_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.PRUNED])
    complete_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.COMPLETE])
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {len(complete_trials)} –∑–∞–≤–µ—Ä—à–µ–Ω–æ, {len(pruned_trials)} –æ—Ç—Å–µ—á–µ–Ω–æ (pruned).")

    best_trial = study.best_trial

    print("\n--- ü•á –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ ---")
    print(f"Trial #{best_trial.number}")

    best_val_loss = best_trial.user_attrs.get('val_loss', 'N/A')
    print(f"  - val_loss: {best_val_loss:.4f}")
    print(f"  - val_accuracy: {best_trial.value:.4f}")
    print("–ù–∞–π–¥–µ–Ω–Ω—ã–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(json.dumps(best_trial.params, indent=4))

    params_path = RESULTS_DIR / f"best_params_{args.model_name}.json"
    with open(params_path, 'w') as f:
        json.dump(best_trial.params, f, indent=4)
    print(f"\n‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏) —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {params_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–π –ø–æ–∏—Å–∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é Optuna.")
    parser.add_argument('--model_name', type=str, required=True, help="–ò–º—è –º–æ–¥–µ–ª–∏, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–π –ø–æ–¥–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")
    parser.add_argument('--n_trials', type=int, default=200, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ (–æ–±—É—á–µ–Ω–∏–π).")
    parser.add_argument('--batch_size', type=int, default=4096)

    args, _ = parser.parse_known_args()
    main(args)