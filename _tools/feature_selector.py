import argparse
import json
import warnings
from pathlib import Path
import pandas as pd
import numpy as np
import catboost as cb
from tqdm import tqdm
from sklearn.model_selection import StratifiedKFold
from sklearn.inspection import permutation_importance
from sklearn.metrics import log_loss
import gc

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π ---
PROJECT_ROOT = Path(__file__).resolve().parents[1]
RESULTS_DIR = PROJECT_ROOT / "2_results"

warnings.filterwarnings("ignore", category=UserWarning, module='catboost')

def load_features_from_files(source_dir: Path, n_files: int = None) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ CSV-—Ñ–∞–π–ª–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏–∑: {source_dir}")
    files = sorted(list(source_dir.glob("*.csv")))
    if not files:
        raise FileNotFoundError(f"–í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {source_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ CSV —Ñ–∞–π–ª–æ–≤.")
    
    if n_files:
        files = files[:n_files]
        
    df_list = [pd.read_csv(f) for f in tqdm(files, desc="–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")]
    return pd.concat(df_list, ignore_index=True)

def filter_by_correlation(df: pd.DataFrame, threshold: float) -> list:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –ø–æ—Ä–æ–≥—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏."""
    if threshold >= 1.0:
        print("–ü–æ—Ä–æ–≥ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ >= 1.0, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è.")
        return df.columns.tolist()
        
    print(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è {df.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ—Ä–æ–≥–æ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ {threshold}...")
    corr_matrix = df.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
    
    kept_features = df.drop(columns=to_drop).columns.tolist()
    print(f"–û—Å—Ç–∞–ª–æ—Å—å {len(kept_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")
    return kept_features

def calculate_loss_importance_for_fold(model, X_val, y_val):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç Permutation Importance –Ω–∞ –æ—Å–Ω–æ–≤–µ logloss –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–æ–ª–¥–∞.
    """
    importances = {}
    baseline_preds = model.predict_proba(X_val)
    baseline_loss = log_loss(y_val, baseline_preds)

    for col in X_val.columns:
        X_val_permuted = X_val.copy()
        
        permuted_values = X_val_permuted[col].to_numpy()
        np.random.shuffle(permuted_values)
        X_val_permuted[col] = permuted_values
        
        permuted_preds = model.predict_proba(X_val_permuted)
        permuted_loss = log_loss(y_val, permuted_preds)
        
        importances[col] = permuted_loss - baseline_loss
        
    return importances

def main(args):
    source_dir = RESULTS_DIR / args.source_dir_name
    
    try:
        df = load_features_from_files(source_dir)
    except FileNotFoundError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}"); return

    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    
    exclude_cols = {'datetime', 'ticker', 'label', 'meta_target', 'open', 'high', 'low', 'close', 'volume'}
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    # --- üëáüëáüëá –ï–î–ò–ù–°–¢–í–ï–ù–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï –ó–î–ï–°–¨ üëáüëáüëá ---
    # –ë—ã–ª –∏—Å–ø—Ä–∞–≤–ª–µ–Ω —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å `FutureWarning` –æ—Ç pandas.
    # –õ–æ–≥–∏–∫–∞ –æ—Å—Ç–∞–ª–∞—Å—å –∞–±—Å–æ–ª—é—Ç–Ω–æ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ–π.
    for col in tqdm(feature_cols, desc="–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤"):
        if df[col].isnull().any():
            # –°—Ç–∞—Ä–∞—è —Å—Ç—Ä–æ–∫–∞: df[col].fillna(df[col].median(), inplace=True)
            df[col] = df[col].fillna(df[col].median())
    # --- üëÜüëÜüëÜ –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–Ø üëÜüëÜüëÜ ---

    X = df[feature_cols]
    y = df['label']
    del df; gc.collect()
    
    all_importances = []
    
    print(f"\n--- üöÄ –ó–∞–ø—É—Å–∫ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –º–µ—Ç—Ä–∏–∫–µ: '{args.ranking_metric.upper()}' ---")
    
    for r in range(args.repeats):
        print(f"\n--- –ü–æ–≤—Ç–æ—Ä [{r + 1}/{args.repeats}] ---")
        skf = StratifiedKFold(n_splits=args.n_splits, shuffle=True, random_state=42 + r)
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            print(f"  –§–æ–ª–¥ [{fold + 1}/{args.n_splits}]...")
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            model = cb.CatBoostClassifier(
                iterations=500, learning_rate=0.05, depth=6,
                loss_function='MultiClass', eval_metric='Accuracy',
                random_seed=42, verbose=0, task_type="GPU" if args.use_gpu else "CPU",
                early_stopping_rounds=50
            )
            
            model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=0)
            
            fold_importances = {}
            if args.ranking_metric == 'loss':
                fold_importances = calculate_loss_importance_for_fold(model, X_val, y_val)
            else: # 'accuracy'
                result = permutation_importance(
                    model, X_val, y_val, n_repeats=1, random_state=42, n_jobs=-1
                )
                for i, feat in enumerate(X_val.columns):
                    fold_importances[feat] = result.importances_mean[i]
            
            all_importances.append(fold_importances)
            del X_train, X_val, y_train, y_val, model; gc.collect()

    ranked_features = pd.DataFrame(all_importances).mean()
    
    if args.ranking_metric == 'loss':
        ranked_features.sort_values(ascending=True, inplace=True)
        print("\n--- –†–µ–π—Ç–∏–Ω–≥ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –í–õ–ò–Ø–ù–ò–Æ –ù–ê LOSS (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ = –≤—Ä–µ–¥–Ω—ã–µ) ---")
        print("   loss_impact < 0: –í—Ä–µ–¥–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ (—É–¥–∞–ª–µ–Ω–∏–µ —É–ª—É—á—à–∞–µ—Ç/—É–º–µ–Ω—å—à–∞–µ—Ç loss)")
        print("   loss_impact > 0: –ü–æ–ª–µ–∑–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ (—É–¥–∞–ª–µ–Ω–∏–µ —É—Ö—É–¥—à–∞–µ—Ç/—É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç loss)")
    else: # 'accuracy'
        ranked_features.sort_values(ascending=False, inplace=True)
        print("\n--- –†–µ–π—Ç–∏–Ω–≥ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –í–õ–ò–Ø–ù–ò–Æ –ù–ê –¢–û–ß–ù–û–°–¢–¨ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ = –ø–æ–ª–µ–∑–Ω—ã–µ) ---")

    kept_after_corr = filter_by_correlation(X[ranked_features.head(args.top_n).index], args.corr_threshold)
    final_feature_list = [feat for feat in ranked_features.index if feat in kept_after_corr]
    
    if len(final_feature_list) > args.top_n:
        final_feature_list = final_feature_list[:args.top_n]

    print(f"\n--- –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥ (—Ç–æ–ø-20) ---")
    print(ranked_features.head(20).to_string())

    output_path = RESULTS_DIR / f"selected_features_{args.model_name}.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_feature_list, f, ensure_ascii=False, indent=4)
        
    print(f"\n‚úÖ –°–ø–∏—Å–æ–∫ –∏–∑ {len(final_feature_list)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Select best features using CatBoost and correlation filtering.")
    parser.add_argument("--model_name", type=str, required=True, help="A unique name for the final selected feature set.")
    parser.add_argument("--source_dir_name", type=str, default="features_processed", 
                        help="Name of the source directory inside 2_results (e.g., 'features_processed' or 'features_combined/your_name').")
    parser.add_argument("--ranking_metric", type=str, default="accuracy", choices=["accuracy", "loss"], 
                        help="Metric to use for permutation importance: 'accuracy' (default) or 'loss'.")
    parser.add_argument("--top_n", type=int, default=160, help="Number of top features to consider for correlation filtering and final selection.")
    parser.add_argument("--corr_threshold", type=float, default=1.0, help="Correlation threshold to filter features.")
    parser.add_argument("--repeats", type=int, default=1, help="Number of times to repeat the cross-validation process for stable importance scores.")
    parser.add_argument("--n_splits", type=int, default=5, help="Number of splits for StratifiedKFold cross-validation.")
    parser.add_argument("--use_gpu", action='store_true', help="Use GPU for CatBoost training.")
    
    args = parser.parse_args()
    main(args)