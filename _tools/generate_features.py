# _tools/generate_features.py
# -*- coding: utf-8 -*-
"""
–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º.
–ó–∞–ø—É—Å–∫:
    python -m _tools.generate_features --universe_file universe.csv --workers 7
"""
from __future__ import annotations

import os
import sys
import argparse
import warnings
from pathlib import Path
from typing import Dict, Tuple, Optional
import inspect

import pandas as pd
from tqdm import tqdm
from joblib import Parallel, delayed
from contextlib import contextmanager

# --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—É—Ç–µ–π –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤ –ø–∞–∫–µ—Ç–∞ ---
ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

# --- –ò–º–ø–æ—Ä—Ç—ã –∏–∑ —è–¥—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ ---
from _core.paths import DATA_DIR, FEATURES_DIR_RAW, ensure_dirs, PROJECT_ROOT
from _core.data_loader import load_data
from _core.feature_generator import (
    create_cross_sectional_features,
    create_individual_features,
)

warnings.simplefilter("ignore", UserWarning)


# --- tqdm-joblib: –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è Parallel ---
@contextmanager
def tqdm_joblib(tqdm_object):
    """Context manager –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ joblib.Parallel —Å tqdm."""
    from joblib import parallel
    class TqdmBatchCompletionCallback(parallel.BatchCompletionCallBack):
        def __call__(self, *args, **kwargs):
            tqdm_object.update(n=self.batch_size)
            return super().__call__(*args, **kwargs)
    old_cb = parallel.BatchCompletionCallBack
    parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback
    try:
        yield tqdm_object
    finally:
        parallel.BatchCompletionCallBack = old_cb
        tqdm_object.close()


def _load_universe(universe_file: str) -> list[str]:
    f = (PROJECT_ROOT / universe_file) if not universe_file.startswith(str(PROJECT_ROOT)) else Path(universe_file)
    if not f.exists():
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –≤—Å–µ–ª–µ–Ω–Ω–æ–π: {f}")
    df = pd.read_csv(f)
    if "ticker" in df.columns:
        vals = df["ticker"].dropna().astype(str).str.strip().tolist()
    else:
        vals = df.iloc[:, 0].dropna().astype(str).str.strip().tolist()
    return [t for t in vals if t]


def _try_load_first(candidates: list[str]) -> Optional[pd.DataFrame]:
    for name in candidates:
        try:
            df = load_data(name)
            if isinstance(df, pd.DataFrame) and not df.empty:
                return df
        except Exception:
            continue
    return None


def _build_external_data() -> pd.DataFrame:
    """
    –°–æ–±–∏—Ä–∞–µ–º –≤–Ω–µ—à–Ω–∏–π DataFrame —Å –∏–Ω–¥–µ–∫—Å–æ–º datetime.
    –§–ò–ö–°: –¥–ª—è 'CBR_KEY_RATE' –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫—É 'key_rate' (–ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –≤ 'close'),
    —á—Ç–æ–±—ã –∏—Ç–æ–≥–æ–≤–æ–µ –∏–º—è —Å—Ç–∞–ª–æ 'cbr_rate_close'.
    """
    sources: Dict[str, list[str]] = {
        "imoex":    ["IMOEX", "IMOEX.csv"],
        "usdrub":   ["USDRUB", "USDRUB.csv", "USD_RUB"],
        "brent":    ["BRENT", "BRENT.csv", "BR"],
        "sp500":    ["SP500", "SP500.csv", "^GSPC", "S&P500"],
        "vix":      ["VIX", "VIX.csv"],
        "cbr_rate": ["CBR_KEY_RATE", "CBR_KEY_RATE.csv", "KEY_RATE"],
    }
    frames = []
    for key, candidates in sources.items():
        df = _try_load_first(candidates)
        if df is None or df.empty:
            continue

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–ª–æ–Ω–∫—É –¥–∞—Ç—ã
        if "datetime" not in df.columns:
            for alt in ["date", "time", "Date", "DATE"]:
                if alt in df.columns:
                    df = df.rename(columns={alt: "datetime"})
                    break

        # –î–ª—è —Å—Ç–∞–≤–∫–∏ –¶–ë: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–æ–ª–æ–Ω–∫–∏ 'key_rate' (–∑–∞–º–µ–Ω–∏–º –Ω–∞ 'close')
        if key == "cbr_rate":
            if "close" not in df.columns and "key_rate" in df.columns:
                df = df.rename(columns={"key_rate": "close"})

        # –û–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        if "datetime" not in df.columns or "close" not in df.columns:
            continue

        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω—ã -> <key>_close
        df = df[["datetime", "close"]].copy()
        df["datetime"] = pd.to_datetime(df["datetime"], errors="coerce")
        df = df.dropna(subset=["datetime"]).sort_values("datetime")
        df = df.rename(columns={"close": f"{key}_close"}).set_index("datetime")
        frames.append(df)

    if not frames:
        return pd.DataFrame(index=pd.DatetimeIndex([], name="datetime"))

    ext = pd.concat(frames, axis=1, join="outer")
    ext = ext[~ext.index.duplicated(keep="first")]
    return ext


def _collect_all_data(tickers: list[str]) -> Dict[str, pd.DataFrame]:
    all_data: Dict[str, pd.DataFrame] = {}
    for t in tqdm(tickers, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"):
        try:
            df = load_data(t)
            if df is not None and not df.empty:
                all_data[t] = df
        except Exception as e:
            warnings.warn(f"[{t}] –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å: {e}")
    return all_data


def _call_create_individual_features(
    item: Tuple[str, pd.DataFrame],
    cs: Optional[pd.DataFrame],
    external_df: Optional[pd.DataFrame],
):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–∑–æ–≤ create_individual_features —Å —É—á—ë—Ç–æ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Å–∏–≥–Ω–∞—Ç—É—Ä—ã –≤ —è–¥—Ä–µ."""
    sig = inspect.signature(create_individual_features)
    params = sig.parameters
    kwargs = {}
    if "external_data" in params:
        kwargs["external_data"] = external_df
    elif "external" in params:
        kwargs["external"] = external_df
    if "cs_features" in params:
        kwargs["cs_features"] = cs
    try:
        return create_individual_features(item, **kwargs)
    except TypeError:
        # –§–æ–ª–ª–±—ç–∫ –Ω–∞ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ (–µ—Å–ª–∏ —è–¥—Ä–æ –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç kwargs)
        pos_args = [item]
        pos_param_names = [
            name for name, p in params.items()
            if p.kind in (p.POSITIONAL_ONLY, p.POSITIONAL_OR_KEYWORD)
        ][1:]
        if pos_param_names:
            if len(pos_param_names) >= 1:
                pos_args.append(external_df)
            if len(pos_param_names) >= 2:
                pos_args.append(cs)
            return create_individual_features(*pos_args)
        raise


def _process_one(item: Tuple[str, pd.DataFrame], cs, external_df):
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞."""
    try:
        t, df_feats = _call_create_individual_features(item, cs, external_df)
        return t, df_feats
    except Exception as e:
        warnings.warn(f"[{item[0]}] –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á—ë—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
        return item[0], None


def main(args: argparse.Namespace) -> None:
    ensure_dirs()
    print("‚Äî" * 60)
    print("üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (_tools.generate_features)")
    print(f"ROOT: {ROOT}")
    print(f"DATA_DIR: {DATA_DIR}")
    print(f"FEATURES_DIR_RAW: {FEATURES_DIR_RAW}")
    print("‚Äî" * 60)

    tickers = _load_universe(args.universe_file)
    if not tickers:
        print("‚ö†Ô∏è –í—Å–µ–ª–µ–Ω–Ω–∞—è –ø—É—Å—Ç–∞ ‚Äî –≤—ã—Ö–æ–¥–∏–º.")
        return

    all_data = _collect_all_data(tickers)
    if not all_data:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–∏–∫–µ—Ä–∞–º ‚Äî –≤—ã—Ö–æ–¥–∏–º.")
        return

    # 1) –ö—Ä–æ—Å—Å-—Å–µ–∫—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    cs = create_cross_sectional_features(all_data)

    # 2) –í–Ω–µ—à–Ω–∏–µ —Ä—è–¥—ã (–≤ —Ç.—á. key_rate -> cbr_rate_close)
    external_df = _build_external_data()

    # 3) –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Äî –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
    items = list(all_data.items())
    n_jobs = max(1, int(args.workers))
    print(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: workers={n_jobs}")

    with tqdm_joblib(tqdm(desc="–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", total=len(items))):
        results = Parallel(n_jobs=n_jobs, backend="loky", prefer="processes")(
            delayed(_process_one)(item, cs, external_df) for item in items
        )

    features: Dict[str, pd.DataFrame] = {t: df for t, df in results if df is not None and not df.empty}
    if not features:
        print("‚ö†Ô∏è –ù–∏ –æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ —Å–æ–∑–¥–∞–Ω–æ ‚Äî –≤—ã—Ö–æ–¥–∏–º.")
        return

    # 4) –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    FEATURES_DIR_RAW.mkdir(parents=True, exist_ok=True)
    for t, df in tqdm(features.items(), desc="–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤"):
        df.reset_index().to_csv(FEATURES_DIR_RAW / f"{t}.csv", index=False)

    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ. –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(features)} ‚Üí {FEATURES_DIR_RAW}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π).")
    parser.add_argument("--universe_file", type=str, default="universe.csv")
    parser.add_argument("--workers", type=int, default=max(1, (os.cpu_count() or 2) - 1))
    main(parser.parse_args())