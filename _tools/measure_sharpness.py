# _tools/measure_sharpness.py

import argparse
import numpy as np
import os
from pathlib import Path
import warnings
import json
import pickle
from tqdm import tqdm # <--- –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–π –∏–º–ø–æ—Ä—Ç

# --- –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π ---
warnings.filterwarnings("ignore", category=UserWarning)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf

# --- –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π ---
try:
    PROJECT_DIR = Path(__file__).resolve().parent.parent
except NameError:
    PROJECT_DIR = Path.cwd()

RESULTS_DIR = PROJECT_DIR / "2_results"
MODELS_DIR = RESULTS_DIR / "models"
SCALERS_DIR = RESULTS_DIR / "scalers"
TFRECORDS_DIR = RESULTS_DIR
# --- –ö–æ–Ω–µ—Ü –±–ª–æ–∫–∞ ---

def parse_tfrecord_fn(example):
    """–ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∏–∑ TFRecord —Ñ–∞–π–ª–∞."""
    feature_description = { 'sequence': tf.io.FixedLenFeature([], tf.string), 'target': tf.io.FixedLenFeature([], tf.int64) }
    example = tf.io.parse_single_example(example, feature_description)
    sequence = tf.io.parse_tensor(example['sequence'], out_type=tf.float32)
    label = example['target']
    return sequence, label

def load_tfrecord_dataset(file_path, batch_size):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ TFRecord —Ñ–∞–π–ª–∞."""
    dataset = tf.data.TFRecordDataset(file_path)
    dataset = dataset.map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)
    # –í–∞–∂–Ω–æ: –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º .repeat() –¥–ª—è –æ—Ü–µ–Ω–∫–∏, –Ω–∞–º –Ω—É–∂–µ–Ω –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –ø–æ –≤—Å–µ–º –¥–∞–Ω–Ω—ã–º
    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

def main(args):
    model_name = args.model_name
    model_path = MODELS_DIR / model_name / "model.keras" 
    scaler_path = SCALERS_DIR / f"{model_name}_scaler.pkl"
    tfrecord_base_path = TFRECORDS_DIR / f"tfrecord_{model_name}"
    meta_path = tfrecord_base_path / "metadata.json"

    print(f"--- üìè –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ —Ä–µ–∑–∫–æ—Å—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–∏: '{model_name}' ---")
    try:
        model = tf.keras.models.load_model(model_path)
        with open(scaler_path, 'rb') as f: scaler = pickle.load(f)
        with open(meta_path, 'r') as f: metadata = json.load(f)
        print("‚úÖ –ú–æ–¥–µ–ª—å, —Å–∫–µ–π–ª–µ—Ä –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: {e}"); return
        
    seq_len, n_features = metadata['lookback_period'], metadata['num_features']

    # --- –ü–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö ---
    val_file = tfrecord_base_path / "test.tfrecord"
    val_dataset = load_tfrecord_dataset(str(val_file), args.batch_size)
    
    @tf.function
    def scale_features(features, label):
        original_shape = tf.shape(features)
        reshaped_features = tf.reshape(features, [-1, n_features])
        scaled_reshaped = tf.py_function(func=scaler.transform, inp=[reshaped_features], Tout=tf.float32)
        scaled_features = tf.reshape(scaled_reshaped, original_shape)
        scaled_features.set_shape([None, seq_len, n_features]); label.set_shape([None])
        return scaled_features, label

    val_dataset_scaled = val_dataset.map(scale_features, num_parallel_calls=tf.data.AUTOTUNE)
    print("‚úÖ –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω.")

    # 1. –°—á–∏—Ç–∞–µ–º –±–∞–∑–æ–≤—ã–π loss –Ω–∞ –≤—Å–µ–º —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
    print("\n–†–∞—Å—á–µ—Ç –±–∞–∑–æ–≤–æ–π –æ—à–∏–±–∫–∏ –Ω–∞ –≤—Å–µ–º —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ...")
    loss_base, acc_base = model.evaluate(val_dataset_scaled, verbose=0)
    print(f"  - –ë–∞–∑–æ–≤—ã–π Loss: {loss_base:.4f}")

    # 2. –ù–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ "—Ç–æ–ª–∫–∞–µ–º" –º–æ–¥–µ–ª—å –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    perturbed_losses = []
    center_weights = model.get_weights()
    
    print(f"\n–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ {args.n_probes} —Ç–µ—Å—Ç–æ–≤—ã—Ö '—Ç–æ–ª—á–∫–æ–≤' –º–æ–¥–µ–ª–∏...")
    for i in tqdm(range(args.n_probes), desc="–ü—Ä–æ–±—ã"):
        # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º, –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–∞–∫ –≤–µ—Å–∞
        noise = [np.random.normal(0, 1, w.shape) for w in center_weights]
        for n, w in zip(noise, center_weights):
            n_norm = np.linalg.norm(n.flatten())
            w_norm = np.linalg.norm(w.flatten())
            if n_norm > 0:
                n *= (w_norm / n_norm) * args.noise_scale
        
        perturbed_weights = [w + n for w, n in zip(center_weights, noise)]
        model.set_weights(perturbed_weights)
        
        loss_perturbed, _ = model.evaluate(val_dataset_scaled, verbose=0)
        perturbed_losses.append(loss_perturbed)

    avg_perturbed_loss = np.mean(perturbed_losses)
    print(f"  - –°—Ä–µ–¥–Ω–∏–π Loss –ø–æ—Å–ª–µ '—Ç–æ–ª—á–∫–æ–≤': {avg_perturbed_loss:.4f}")

    # 3. –°—á–∏—Ç–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å —Ä–µ–∑–∫–æ—Å—Ç–∏
    sharpness_index = avg_perturbed_loss - loss_base

    print("\n--- üìä –ò—Ç–æ–≥–∏ ---")
    print(f"–ò–Ω–¥–µ–∫—Å —Ä–µ–∑–∫–æ—Å—Ç–∏ (—á–µ–º –Ω–∏–∂–µ, —Ç–µ–º –ª—É—á—à–µ): {sharpness_index:.4f}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ —Ä–µ–∑–∫–æ—Å—Ç–∏ (—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏) –º–∏–Ω–∏–º—É–º–∞ –º–æ–¥–µ–ª–∏.")
    parser.add_argument('--model_name', type=str, required=True)
    parser.add_argument('--batch_size', type=int, default=4096)
    parser.add_argument('--n_probes', type=int, default=10, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª—É—á–∞–π–Ω—ã—Ö '—Ç–æ–ª—á–∫–æ–≤' –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.")
    parser.add_argument('--noise_scale', type=float, default=1e-3, help="–ú–∞—Å—à—Ç–∞–± —à—É–º–∞ –¥–ª—è '—Ç–æ–ª—á–∫–∞' (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–æ—Ä–º—ã –≤–µ—Å–æ–≤).")
    args = parser.parse_args()
    main(args)